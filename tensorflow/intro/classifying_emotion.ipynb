{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import zipfile \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-08-11 20:58:34--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/happy-or-sad.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 34.101.5.112, 34.101.5.80, 34.101.5.48, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|34.101.5.112|:443... connected.\n",
      "WARNING: cannot verify storage.googleapis.com's certificate, issued by 'CN=GTS CA 1C3,O=Google Trust Services LLC,C=US':\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2670333 (2.5M) [application/zip]\n",
      "Saving to: 'C:/Clarenti/Data/Project/ML/Program/Dataset/Emotion/happy-or-sad.zip'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1% 3.15M 1s\n",
      "    50K .......... .......... .......... .......... ..........  3% 4.03M 1s\n",
      "   100K .......... .......... .......... .......... ..........  5% 5.07M 1s\n",
      "   150K .......... .......... .......... .......... ..........  7% 4.19M 1s\n",
      "   200K .......... .......... .......... .......... ..........  9% 4.70M 1s\n",
      "   250K .......... .......... .......... .......... .......... 11% 5.57M 1s\n",
      "   300K .......... .......... .......... .......... .......... 13% 4.77M 1s\n",
      "   350K .......... .......... .......... .......... .......... 15% 1.64M 1s\n",
      "   400K .......... .......... .......... .......... .......... 17% 5.68M 1s\n",
      "   450K .......... .......... .......... .......... .......... 19% 3.52M 1s\n",
      "   500K .......... .......... .......... .......... .......... 21% 3.96M 1s\n",
      "   550K .......... .......... .......... .......... .......... 23% 5.91M 1s\n",
      "   600K .......... .......... .......... .......... .......... 24% 4.36M 0s\n",
      "   650K .......... .......... .......... .......... .......... 26% 4.67M 0s\n",
      "   700K .......... .......... .......... .......... .......... 28% 5.59M 0s\n",
      "   750K .......... .......... .......... .......... .......... 30% 4.33M 0s\n",
      "   800K .......... .......... .......... .......... .......... 32% 1.30M 0s\n",
      "   850K .......... .......... .......... .......... .......... 34% 4.52M 0s\n",
      "   900K .......... .......... .......... .......... .......... 36% 5.62M 0s\n",
      "   950K .......... .......... .......... .......... .......... 38% 4.88M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 40% 6.22M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 42% 28.0M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 44% 16.2M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 46% 28.1M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 47% 30.3M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 49% 33.8M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 51% 34.2M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 53% 28.0M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 55% 32.6M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 57% 29.0M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 59% 32.8M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 61% 25.1M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 63% 18.1M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 65% 34.5M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 67% 29.9M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 69% 29.3M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 70% 30.0M 0s\n",
      "  1850K .......... .......... .......... .......... .......... 72% 37.1M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 74% 32.2M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 76% 24.4M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 78% 31.7M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 80% 36.9M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 82% 18.7M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 84% 23.7M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 86% 35.9M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 88% 32.1M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 90% 26.1M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 92% 27.3M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 93% 33.7M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 95% 33.9M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 97% 19.4M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 99% 7.38M 0s\n",
      "  2600K .......                                               100% 19.1M=0.3s\n",
      "\n",
      "2022-08-11 20:58:36 (7.78 MB/s) - 'C:/Clarenti/Data/Project/ML/Program/Dataset/Emotion/happy-or-sad.zip' saved [2670333/2670333]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/happy-or-sad.zip\" -O \"C:\\Clarenti\\Data\\Project\\ML\\Program\\Dataset\\Emotion\\happy-or-sad.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseFile = \"C:\\\\Clarenti\\\\Data\\\\Project\\ML\\\\Program\\\\Dataset\\\\Emotion\"\n",
    "\n",
    "# UNZIP\n",
    "local_zip = zipfile.ZipFile(os.path.join(BaseFile,\"happy-or-sad.zip\"), 'r')\n",
    "local_zip.extractall(BaseFile)\n",
    "local_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "happy_dir = os.path.join(BaseFile, \"happy\")\n",
    "sad_dir = os.path.join(BaseFile, \"sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255.,\n",
    "    horizontal_flip=0.2,\n",
    "    vertical_flip=0.2,\n",
    "    shear_range=0.1,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    BaseFile,\n",
    "    target_size=(150,150),\n",
    "    class_mode='binary',\n",
    "    batch_size=25,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    BaseFile,\n",
    "    target_size=(150,150),\n",
    "    class_mode='binary',\n",
    "    batch_size=25,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1 (Conv2D)              (None, 148, 148, 128)     3584      \n",
      "                                                                 \n",
      " Pool1 (MaxPooling2D)        (None, 74, 74, 128)       0         \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 74, 74, 128)       0         \n",
      "                                                                 \n",
      " Conv2 (Conv2D)              (None, 72, 72, 64)        73792     \n",
      "                                                                 \n",
      " Pool2 (MaxPooling2D)        (None, 36, 36, 64)        0         \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 36, 36, 64)        0         \n",
      "                                                                 \n",
      " Conv3 (Conv2D)              (None, 34, 34, 32)        18464     \n",
      "                                                                 \n",
      " Pool3 (MaxPooling2D)        (None, 17, 17, 32)        0         \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 17, 17, 32)        0         \n",
      "                                                                 \n",
      " Conv4 (Conv2D)              (None, 15, 15, 32)        9248      \n",
      "                                                                 \n",
      " Pool4 (MaxPooling2D)        (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 1568)              0         \n",
      "                                                                 \n",
      " Hidden1 (Dense)             (None, 512)               803328    \n",
      "                                                                 \n",
      " Hidden2 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 974,209\n",
      "Trainable params: 974,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    Conv2D(name=\"Conv1\", filters=128, kernel_size=(3,3), activation='relu', input_shape=(150,150,3)),\n",
    "    MaxPooling2D(name=\"Pool1\", pool_size=(2,2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(name=\"Conv2\", filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    MaxPooling2D(name=\"Pool2\", pool_size=(2,2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(name=\"Conv3\", filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    MaxPooling2D(name=\"Pool3\", pool_size=(2,2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(name=\"Conv4\", filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    MaxPooling2D(name=\"Pool4\", pool_size=(2,2)),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(name=\"Hidden1\", units=512, activation='relu'),\n",
    "    Dense(name=\"Hidden2\", units=128, activation='relu'),\n",
    "    Dense(name=\"Output\", units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 9s 2s/step - loss: 1.2626 - accuracy: 0.3750 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.8209 - accuracy: 0.5156 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.6962 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 7s 3s/step - loss: 0.6831 - accuracy: 0.7344 - val_loss: 0.6922 - val_accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 8s 2s/step - loss: 0.6714 - accuracy: 0.5156 - val_loss: 0.6893 - val_accuracy: 0.5000\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.7456 - accuracy: 0.5781 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.6089 - accuracy: 0.8594 - val_loss: 0.7009 - val_accuracy: 0.5000\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.5241 - accuracy: 0.6875 - val_loss: 0.8774 - val_accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 6s 3s/step - loss: 0.7136 - accuracy: 0.5781 - val_loss: 0.6903 - val_accuracy: 0.4375\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.4070 - accuracy: 0.9531 - val_loss: 0.8282 - val_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.6033 - accuracy: 0.7500 - val_loss: 0.6602 - val_accuracy: 0.5625\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.4278 - accuracy: 0.7812 - val_loss: 0.6217 - val_accuracy: 0.6250\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.2092 - accuracy: 1.0000 - val_loss: 0.5461 - val_accuracy: 0.7500\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 7s 3s/step - loss: 0.3130 - accuracy: 0.8750 - val_loss: 0.6541 - val_accuracy: 0.5625\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0680 - accuracy: 1.0000 - val_loss: 0.9015 - val_accuracy: 0.5625\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.8561 - val_accuracy: 0.6250\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0302 - accuracy: 1.0000 - val_loss: 0.9042 - val_accuracy: 0.5000\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 7s 3s/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.9509 - val_accuracy: 0.6250\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.1135 - val_accuracy: 0.5625\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.9723 - val_accuracy: 0.5625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18800d75bb0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the accuracy is not changing, it means the optimizer has found a local minimum for the loss. \n",
    "# This may be an undesirable minimum. One common local minimum is to always predict the class with the most number of data points.\n",
    "# You should use weighting on the classes to avoid this minimum.\n",
    "\n",
    "# from sklearn.utils import compute_class_weight\n",
    "# classWeight = compute_class_weight('balanced', np.unique(y), y) \n",
    "# classWeight = dict(enumerate(classWeight))\n",
    "\n",
    "model.fit(train_generator, validation_data=validation_generator, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dbf1d3937cd9af2ec7a992e3a66647a7d94267f9ee808a22d5489bdc356721a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
