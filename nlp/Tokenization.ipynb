{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "XRagq7alZAUg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "agqtxCl1Y-ER"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = [\n",
        "    \"I love my rabbit\",\n",
        "    \"I love my dog\",\n",
        "    \"They live happily\",\n",
        "    \"Do you think my dog is cute?\"\n",
        "]"
      ],
      "metadata": {
        "id": "a_A-PsxGZN3I"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OOV Token**: is a word_index that will be used for a word that doesnt exist in a provided vocab list.\n",
        "\n",
        "**num_words**: Maximum number of words to keep in **vocab list**. Only the most common ***num_words-1*** words will be kept.\n",
        "\n",
        "nb. The order of the word index is based on its frequency."
      ],
      "metadata": {
        "id": "5CXqx3sudJ5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only word with <= [num_words-1] index that will be saved in vocab_list/dict.\n",
        "# Word that is not saved into vocab_list will be considered as OOV with index = 1\n",
        "\n",
        "tokenizer = Tokenizer(num_words=3, oov_token=\"<OOV>\") # >>> dict will hold the items which is word with index <= (num - 1) index\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "word_count = tokenizer.word_counts\n",
        "print(word_index)\n",
        "print(word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y02QmNCqtUd",
        "outputId": "0acff803-dc6d-40f6-e70b-e9f0d4ef744d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'i': 3, 'love': 4, 'dog': 5, 'rabbit': 6, 'they': 7, 'live': 8, 'happily': 9, 'do': 10, 'you': 11, 'think': 12, 'is': 13, 'cute': 14}\n",
            "OrderedDict([('i', 2), ('love', 2), ('my', 3), ('rabbit', 1), ('dog', 2), ('they', 1), ('live', 1), ('happily', 1), ('do', 1), ('you', 1), ('think', 1), ('is', 1), ('cute', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"I love my lovely dog\",\n",
        "    \"I love my adorable cat\"\n",
        "]\n",
        "\n",
        "encoded_test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
        "print(encoded_test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQv4CIKGs6SV",
        "outputId": "169ac0aa-92a3-4adc-8ea0-70210d405d20"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 1, 2, 1, 1], [1, 1, 2, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Because we only defined the num_words is 3, which means the dictionary only saved the words with index 1 (OOV) and 2 (my). So, when we test, model will recognize a word out of dict as 'OOV'.  \n",
        "\n",
        "> This num_words is good for large data to reduce (giving) the load on model.\n",
        "Better to do preprocessing text such as lemmatization/stemming, stopword removal for unnecessary words."
      ],
      "metadata": {
        "id": "COOH5ZP2tf7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding and Truncating\n"
      ],
      "metadata": {
        "id": "EIURtTSHvEy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding is to make the encoded_seq be of the same length. </br>\n",
        "Truncating is to truncate the words based on predetermined length."
      ],
      "metadata": {
        "id": "l1ccoGJhzKc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "DcprYL9Lu4QM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_train_seq = tokenizer.texts_to_sequences(train_sentences)\n",
        "\n",
        "padded = pad_sequences(\n",
        "    encoded_train_seq, \n",
        "    padding='post', # default: 'pre'\n",
        "    truncating='post',\n",
        "    maxlen=5)\n",
        "\n",
        "padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yg3PgMVvX9U",
        "outputId": "6c2cf7de-ec09-4873-fb40-d499310b79b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 2, 1, 0],\n",
              "       [1, 1, 2, 1, 0],\n",
              "       [1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 2, 1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding **'post'**: Pad with '0' after the sequence. </br>\n",
        "Truncating **'post'**: Truncate any sequences longer than maxlen."
      ],
      "metadata": {
        "id": "-YYVg_b7zwV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> explanation: maxlen=5, **pad='post'**, ***truncate='post'*** </br>\n",
        "train_sentences = [ </br>\n",
        "    \"I love my rabbit\", -> 4 words; [1, 1, 2, 1, **0**] </br>\n",
        "    \"I love my dog\", -> 4 words [1, 1, 2, 1, **0**]</br>\n",
        "    \"They live happily\", -> 3 words [1, 1, 1, **0, 0**]</br>\n",
        "    \"Do you think my dog is cute?\" -> 7 words [1, 1, 1, 2, 1 | ***1***, ***1***]</br>\n",
        "]"
      ],
      "metadata": {
        "id": "SX1t_f_81sxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Code"
      ],
      "metadata": {
        "id": "tuxItluP0vE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "train_sentences = [\n",
        "    \"I love my rabbit\",\n",
        "    \"My rabbit is beautiful as always\",\n",
        "    \"And the dog is very lovely\",\n",
        "    \"Having them in your side is such a blessing .\"\n",
        "]\n",
        "\n",
        "test_sentences = [\n",
        "    \"Where did you find your dog ?\",\n",
        "    \"Why do you love them ?\",\n",
        "    \"Your dog is indeed very lovely!\"\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=100, \n",
        "    filters=\".\", # only '.' will be filtered from the sentence\n",
        "    char_level=False, # every char won't be treated as a token\n",
        "    oov_token='<OOV>'\n",
        ")\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "# Check on training data\n",
        "encoded_train_seq = tokenizer.texts_to_sequences(train_sentences)\n",
        "print(\"encoded_train_seq : \", encoded_train_seq)\n",
        "\n",
        "# tokenizer.fit_on_sequences(encoded_train_seq)\n",
        "\n",
        "decoded_train_seq = tokenizer.sequences_to_texts(encoded_train_seq)\n",
        "print(\"decoded_train_seq : \", decoded_train_seq)\n",
        "\n",
        "# APPLY PADDING AND TRUNCATING ON ENCODED SEQ\n",
        "maxlen = max(len(i) for i in encoded_train_seq) # >>> 9\n",
        "train_padded_seq = pad_sequences(\n",
        "    encoded_train_seq,\n",
        "    maxlen=maxlen,\n",
        "    padding='post',\n",
        "    truncating='post',\n",
        "    value=0\n",
        ")\n",
        "print(train_padded_seq)\n",
        "\n",
        "decoded_train_padded_seq = tokenizer.sequences_to_texts(train_padded_seq)\n",
        "print(\"decoded_train_padded_seq : \", decoded_train_padded_seq)\n",
        "\n",
        "# TESTING\n",
        "# tokenizer.fit_on_texts(test_sentences)*\n",
        "encoded_test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
        "print(\"encoded_test_seq : \", encoded_test_seq)\n",
        "\n",
        "maxlen = max(len(i) for i in encoded_test_seq)\n",
        "test_padded_seq = pad_sequences(\n",
        "    encoded_test_seq,\n",
        "    maxlen=maxlen,\n",
        "    padding='post',\n",
        "    truncating='post',\n",
        "    value=0\n",
        ")\n",
        "print(test_padded_seq)\n",
        "\n",
        "# Check on testing data\n",
        "decoded_test_padded_seq = tokenizer.sequences_to_texts(test_padded_seq)\n",
        "print(\"decoded_test_padded_seq : \", decoded_test_padded_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8kRIy4X6qN0",
        "outputId": "9a229d25-d948-4f44-89aa-b94c78257e12"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded_train_seq :  [[5, 6, 3, 4], [3, 4, 2, 7, 8, 9], [10, 11, 12, 2, 13, 14], [15, 16, 17, 18, 19, 2, 20, 21, 22]]\n",
            "decoded_train_seq :  ['i love my rabbit', 'my rabbit is beautiful as always', 'and the dog is very lovely', 'having them in your side is such a blessing']\n",
            "[[ 5  6  3  4  0  0  0  0  0]\n",
            " [ 3  4  2  7  8  9  0  0  0]\n",
            " [10 11 12  2 13 14  0  0  0]\n",
            " [15 16 17 18 19  2 20 21 22]]\n",
            "decoded_train_padded_seq :  ['i love my rabbit <OOV> <OOV> <OOV> <OOV> <OOV>', 'my rabbit is beautiful as always <OOV> <OOV> <OOV>', 'and the dog is very lovely <OOV> <OOV> <OOV>', 'having them in your side is such a blessing']\n",
            "encoded_test_seq :  [[1, 1, 1, 1, 18, 12, 1], [1, 1, 1, 6, 16, 1], [18, 12, 2, 1, 13, 1]]\n",
            "[[ 1  1  1  1 18 12  1]\n",
            " [ 1  1  1  6 16  1  0]\n",
            " [18 12  2  1 13  1  0]]\n",
            "decoded_test_padded_seq :  ['<OOV> <OOV> <OOV> <OOV> your dog <OOV>', '<OOV> <OOV> <OOV> love them <OOV> <OOV>', 'your dog is <OOV> very <OOV> <OOV>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> model only detects the word that available in dictionary."
      ],
      "metadata": {
        "id": "m0UjvcE-I1Qb"
      }
    }
  ]
}